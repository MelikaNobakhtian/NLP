{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPDatasetGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installing required packages"
      ],
      "metadata": {
        "id": "1qqMtkCgJri5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-BH8E_NFHsF"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "!pip install img2dataset"
      ],
      "metadata": {
        "id": "u_SgRq_ZGxrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Imports & mounting Google Drive"
      ],
      "metadata": {
        "id": "7GtiJ-P-J3fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "9ux1mxm6JjNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import errno\n",
        "from multiprocessing import Pool\n",
        "import wget\n",
        "import ssl\n",
        "import time\n",
        "import urllib\n",
        "from urllib.request import urlopen\n",
        "from img2dataset import download\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ],
      "metadata": {
        "id": "oy6_qBugFv3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xhObKIoTQDlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Dataset Generation"
      ],
      "metadata": {
        "id": "-o9OXKu0q8NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_caption_pair(args):\n",
        "    global searched_categories\n",
        "    page_url = args\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "    driver.get(page_url)\n",
        "    try:\n",
        "      info = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, \"photo__info\")))\n",
        "    except:\n",
        "      print(f\"Couldn't get page with url: {page_url}. will add it to the unsuccessful page loads...\")\n",
        "      return page_url\n",
        "    title = info.find_element(by=By.TAG_NAME, value=\"h1\").text\n",
        "    caption = info.find_element(by=By.TAG_NAME, value=\"p\").text\n",
        "    tags = driver.find_elements(by=By.CLASS_NAME, value=\"photo__meta\")[1].find_elements(by=By.TAG_NAME, value=\"a\")\n",
        "    for tag in tags:\n",
        "      if tag.text in searched_categories:\n",
        "        return None\n",
        "    image_holder = driver.find_element(by=By.CLASS_NAME, value=\"photo__centered-frame\")\n",
        "    image_url = image_holder.find_element(by=By.TAG_NAME, value = \"img\").get_attribute(\"src\")\n",
        "    main_url = re.search('[^\\?]*', image_url).group()\n",
        "    driver.quit()\n",
        "    return {\"title\": title, \"image_url\": main_url, \"caption\": caption}"
      ],
      "metadata": {
        "id": "W17B_X-oGZTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_json_file(new_data):\n",
        "  with open(f'{dataset_path}/dataset.json', 'r', encoding='utf-8') as f:\n",
        "    try: \n",
        "        data = json.load(f)\n",
        "    except ValueError: \n",
        "         data = []\n",
        "  with open(f'{dataset_path}/dataset.json', 'w', encoding='utf-8') as f:\n",
        "    data = data + new_data\n",
        "    json.dump(data, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "ynbGNGHXXz32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unsuccessful_page_loads(url):\n",
        "  with open(f\"{dataset_path}/unsuccessful_page_loads.txt\", \"a\") as f:\n",
        "    f.write(url+\"\\n\")"
      ],
      "metadata": {
        "id": "AD_1XJrfTger"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the \"raw\" folder along with dataset.json in it must exist in order for this to work:\n",
        "def download_images():\n",
        "  global dataset_path\n",
        "\n",
        "  if os.path.exists(f\"{dataset_path}/images\"):\n",
        "    print(f\"the folder {dataset_path}/images exists. removing it...\")\n",
        "    shutil.rmtree(f\"{dataset_path}/images\")\n",
        "\n",
        "  try:\n",
        "    os.mkdir(os.path.join(dataset_path, \"images\"))\n",
        "  except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "      raise\n",
        "\n",
        "  with open(f'{dataset_path}/dataset.json', 'r', encoding='utf-8') as f:\n",
        "    try: \n",
        "      data = json.load(f)\n",
        "    except ValueError: \n",
        "      data = []\n",
        "  with open(\"urls.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join([d[\"image_url\"] for d in data]))\n",
        "  download(\n",
        "          url_list=\"urls.txt\",\n",
        "          output_folder= dataset_path+\"/images\",\n",
        "          resize_mode=\"keep_ratio\",\n",
        "          image_size=800,\n",
        "          resize_only_if_bigger=True,\n",
        "          retries = 2\n",
        "        )"
      ],
      "metadata": {
        "id": "oOlUBgVAbXJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_image_caption_pair(data, last_id):\n",
        "  tmp = []\n",
        "  for d in data:\n",
        "    if d == None:\n",
        "      continue\n",
        "    elif isinstance(d, str):\n",
        "      unsuccessful_page_loads(d)\n",
        "    else:\n",
        "      tmp.append({\"id\":last_id, \"title\": d[\"title\"], \"image_url\": d[\"image_url\"], \"caption\": d[\"caption\"]})\n",
        "      last_id += 1\n",
        "  return tmp, last_id"
      ],
      "metadata": {
        "id": "m6vfQHUXLBfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_with_categories():\n",
        "    global dataset_path\n",
        "    global searched_categories\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "\n",
        "    categories = [\"Nature\", \"Seasons\", \"Travel\", \"City\", \"Food\", \"Pets\", \"Animal\", \"Product\", \"Business\", \"Education\", \n",
        "                  \"People\", \"Children\", \"Accessories\", \"Medical\", \"Transportation\"]\n",
        "    \n",
        "    #create a \"raw\" folder in the folder specified by user\n",
        "    path = os.path.join(dataset_path, \"raw\")\n",
        "    dataset_path = path\n",
        "    if os.path.exists(f\"{dataset_path}/dataset.json\"):\n",
        "        print(f\"the file {dataset_path}/dataset.json exists. removing it...\")\n",
        "        os.remove(f\"{dataset_path}/dataset.json\")\n",
        "        \n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError as e:\n",
        "        if e.errno != errno.EEXIST:\n",
        "            raise\n",
        "    \n",
        "    if os.path.exists(f\"{dataset_path}/unsuccessful_page_loads.txt\"):\n",
        "        print(f\"the file {dataset_path}/unsuccessful_page_loads.txt exists. removing it...\")\n",
        "        os.remove(f\"{dataset_path}/unsuccessful_page_loads.txt\")\n",
        "    \n",
        "    #creating dataset.json and unsuccessful_page_loads.txt files\n",
        "    open(f\"{dataset_path}/dataset.json\",'w+').close()\n",
        "    open(f\"{dataset_path}/unsuccessful_page_loads.txt\",'w+').close()\n",
        "\n",
        "    pages_data = []\n",
        "    searched_categories = []\n",
        "    last_id = 0\n",
        "    page_first = 1\n",
        "    for category in categories:\n",
        "      for page in range(page_first,200):\n",
        "          print(f\"Getting data from page {page} : https://burst.shopify.com/{category}?page={page}\")\n",
        "          driver.get(f\"https://burst.shopify.com/{category}?page={page}\")\n",
        "          main = driver.find_element(by=By.ID, value=\"Main\")\n",
        "          grid = main.find_element(by=By.TAG_NAME, value=\"section\").find_element(by=By.CLASS_NAME, value=\"js-masonry-grid\")\n",
        "          links = [element.get_attribute(\"href\") for element in grid.find_elements(by =By.TAG_NAME, value=\"a\")]\n",
        "          minibatch_length = 10\n",
        "          minibatch = int(len(links) / minibatch_length)\n",
        "          for iter in tqdm(range(minibatch)):\n",
        "              with Pool() as p:\n",
        "                  data = p.map(get_image_caption_pair, links[iter*minibatch_length:iter*minibatch_length + minibatch_length])\n",
        "              data, last_id = handle_image_caption_pair(data, last_id)\n",
        "              pages_data= pages_data + data\n",
        "          remainingdata = len(links)-minibatch*minibatch_length\n",
        "          if remainingdata != 0:\n",
        "              with Pool() as p:\n",
        "                  data = p.map(get_image_caption_pair, links[minibatch*minibatch_length:])\n",
        "              data, last_id = handle_image_caption_pair(data, last_id)\n",
        "              pages_data= pages_data + data\n",
        "          save_to_json_file(pages_data)\n",
        "          pages_data = []\n",
        "          pagination = main.find_element(by=By.CLASS_NAME, value=\"pagination\")\n",
        "          if len(pagination.find_elements(by=By.LINK_TEXT, value=\"Next ›\")) == 0:\n",
        "            break #this category's pages ended\n",
        "      searched_categories.append(category)\n",
        "      page_first = 1\n",
        "    driver.quit()\n",
        "    print(\"=============================================\")\n",
        "    print(\"json file is ready!\")\n",
        "    \n",
        "    if image_format == \"file\":\n",
        "        print(\"Beginning to save the images in\"+ dataset_path + '/images.')\n",
        "        download_images()\n"
      ],
      "metadata": {
        "id": "AL9l5mYV3REC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###The path where dataset folder is created on\n",
        "dataset_path = \"/content/drive/MyDrive\"\n",
        "#### change this to \"file\" if you want to download and save the images locally after dataset.json is made. \n",
        "# you can also call download_images() for this purpose.\n",
        "image_format = \"url\"\n",
        "\n",
        "searched_categories = []\n",
        "crawl_with_categories()"
      ],
      "metadata": {
        "id": "lcYSxa8Ao3Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_images()"
      ],
      "metadata": {
        "id": "Lw3fePHqrr8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}