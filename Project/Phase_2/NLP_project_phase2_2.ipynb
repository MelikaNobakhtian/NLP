{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Model for Image Captioning"
      ],
      "metadata": {
        "id": "txO7gzAoP1EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive"
      ],
      "metadata": {
        "id": "acoLy7rqMTTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U57nhNlI8Ul7",
        "outputId": "a73f1328-428b-4f3a-adfe-da9bca646b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Images and Dataset files"
      ],
      "metadata": {
        "id": "r0zZWk9BMd_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBHVPUotJAZl"
      },
      "outputs": [],
      "source": [
        "!mkdir ImageCaption_Dataset\n",
        "!unzip /content/drive/MyDrive/Image_Caption/NLP_dataset.zip -d ImageCaption_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "z7cnpEloMji-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SghITBL3_6SU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import keras\n",
        "import random\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4xxPTALyMMg8"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/ImageCaption_Dataset/cleaned_dataset.json\", 'r') as f:\n",
        "    dataset_json = json.load(f)\n",
        "\n",
        "dataset_len = len(dataset_json)\n",
        "dataset_ids = [*range(dataset_len)]\n",
        "\n",
        "#shuffle dataset\n",
        "random.Random(6).shuffle(dataset_ids)\n",
        "\n",
        "captions = [dataset_json[id][\"caption\"] for id in dataset_ids]\n",
        "image_paths = ['/content/ImageCaption_Dataset/images/{0:09d}.jpg'.format(id) for id in dataset_ids]\n",
        "\n",
        "\n",
        "#take 80% as train set, and 20% as test\n",
        "train_length = int(dataset_len * .8)\n",
        "test_length = dataset_len - train_length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Images and Inception Model "
      ],
      "metadata": {
        "id": "ActN9bmYMw_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iNlPZr48HtRr"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_jpeg(img, channels=3)\n",
        "  img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  return img, image_path\n",
        "\n",
        "def create_inceptionv3_model():\n",
        "  inception_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "  return tf.keras.Model(inception_model.input, inception_model.layers[-1].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get features of Images"
      ],
      "metadata": {
        "id": "SciRPhHuNZq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5Y5MgIMHxgn"
      },
      "outputs": [],
      "source": [
        "#get image features from inceptionv3 model's last layer\n",
        "img_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "img_dataset = img_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(32)\n",
        "\n",
        "feature_extraction_model = create_inceptionv3_model()\n",
        "\n",
        "path_of_feature = '/content/drive/MyDrive/Image_Caption/ExtractedFeatures/'\n",
        "\n",
        "i = 0\n",
        "\n",
        "for img, path in tqdm(img_dataset): \n",
        "  features = feature_extraction_model(img)\n",
        "  image_features = tf.reshape(features, (features.shape[0], -1, features.shape[-1]))\n",
        "  \n",
        "\n",
        "  for bf, p in zip(image_features, path):\n",
        "    path_sp = str(p).split('/')[-1]\n",
        "    path_image = path_of_feature + path_sp.split('.')[0] + '.npy'\n",
        "    np.save(path_image, bf.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define tokenizer"
      ],
      "metadata": {
        "id": "k8Hqlvn2Nhm2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jDN19m0eHFX"
      },
      "outputs": [],
      "source": [
        "def get_longest_caption_length(captions):\n",
        "  return max([len(caption.split()) for caption in captions])\n",
        "\n",
        "output_sequence_length = get_longest_caption_length(captions)\n",
        "train_captions = captions[:train_length] \n",
        "\n",
        "# create a tokenizer and get vocabulary from train captions using TextVectorization\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "  standardize=None, \n",
        "  max_tokens=5000,\n",
        "  output_sequence_length=output_sequence_length)\n",
        "\n",
        "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n",
        "tokenizer.adapt(caption_dataset)\n",
        "\n",
        "#get word2index and index2word for mapping the words and indices\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Training Parameters"
      ],
      "metadata": {
        "id": "zdTZg1dHOk21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA7dFdx8fE1l"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "embedding_dim = 512\n",
        "EPOCHS = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset and Tokenize Captions"
      ],
      "metadata": {
        "id": "i2ZuW1W0Ot0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qDy0tuJfNwX"
      },
      "outputs": [],
      "source": [
        "# get tokenized vectors\n",
        "def get_vectors(caps):\n",
        "  cap_dataset = tf.data.Dataset.from_tensor_slices(caps)\n",
        "  return cap_dataset.map(lambda x: tokenizer(x)) #CHANGE THIS TO A [] LATER\n",
        "\n",
        "def create_dataset_data(image_path, caption):\n",
        "  cap_vec = tokenizer(caption)\n",
        "  img_vec = np.load(image_path.decode('utf-8'))\n",
        "  return img_vec, cap_vec\n",
        "\n",
        "def set_dataset_shapes(img_vec, cap_vec):\n",
        "  cap_vec.set_shape(cap_vec.shape)\n",
        "  img_vec.set_shape(img_vec.shape)\n",
        "  return img_vec, cap_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdPH8L3Liw7e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "image_npy_paths = ['/content/drive/MyDrive/Image_Caption/ExtractedFeatures/' + path for path in os.listdir('/content/drive/MyDrive/Image_Caption/ExtractedFeatures/')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwIMvUYkfbYM"
      },
      "outputs": [],
      "source": [
        "train_image_paths = image_npy_paths[:train_length]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_captions))\n",
        "train_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          create_dataset_data, [item1, item2], [tf.float32, tf.int64]),\n",
        "          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.map(\n",
        "          set_dataset_shapes,\n",
        "          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "GjpSFr3pO5Nm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXw3bTYk3D-b"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(512,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.out_fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  \n",
        "  def call(self, x, hidden, features):\n",
        "    x = self.embedding(x)  \n",
        "\n",
        "    x, new_hidden, _= self.lstm2(x)\n",
        "\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    x = self.out_fc(x)\n",
        "\n",
        "    return x, new_hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Encoder"
      ],
      "metadata": {
        "id": "t-B6XpoQPJn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiLwQKDaNzxU"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(keras.Model):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(ImageEncoder, self).__init__()\n",
        "    self.out_fc = tf.keras.layers.Dense(embedding_dim,activation='relu')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.out_fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQWKIlKm3_Dv"
      },
      "outputs": [],
      "source": [
        "Image_encoder = ImageEncoder(embedding_dim)\n",
        "decoder = Decoder(tokenizer.vocabulary_size(), embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning Model"
      ],
      "metadata": {
        "id": "3Ab_XmRYPaCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALN-M3ZL3sGQ"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(keras.Model):\n",
        "  def __init__(\n",
        "        self, encoder, decoder, units\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self.units = units\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "    self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "\n",
        "  def calculate_loss(self, y_true, y_pred, mask):\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    mask_reduced = tf.reduce_sum(mask)\n",
        "    if mask_reduced == 0:\n",
        "      return 0\n",
        "    return tf.reduce_sum(loss) / mask_reduced\n",
        "\n",
        "  def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "    accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=-1))\n",
        "    accuracy = tf.math.logical_and(mask, accuracy)\n",
        "    accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask_reduced = tf.reduce_sum(mask)\n",
        "    if mask_reduced == 0:\n",
        "      return -1\n",
        "    return tf.reduce_sum(accuracy) / mask_reduced\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    # We need to list our metrics here so the reset_states() can be\n",
        "    # called automatically.\n",
        "    return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "  def train_step(self, batch_data):\n",
        "    batch_img, batch_seq = batch_data\n",
        "    batch_loss = 0\n",
        "    batch_acc = []\n",
        "\n",
        "    batch_seq_size = batch_seq.get_shape().as_list()[0] # tf.shape(batch_seq)[0]\n",
        "    hidden = tf.zeros((batch_seq_size, self.units))\n",
        "    decoder_input = tf.expand_dims([word_to_index('<start>')] * batch_seq_size, 1)\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_out = self.encoder(batch_img, training=True)\n",
        "      for i in range(1, batch_seq.shape[1]):\n",
        "        predictions, hidden = self.decoder(decoder_input, hidden, encoder_out)\n",
        "        mask = tf.math.not_equal(batch_seq[:,i], 0)\n",
        "        loss = self.calculate_loss(batch_seq[:,i], predictions, mask)\n",
        "        # print(loss)\n",
        "        batch_loss += loss\n",
        "        acc = self.calculate_accuracy(batch_seq[:,i], predictions, mask)\n",
        "        if acc != -1:\n",
        "          batch_acc.append(acc) \n",
        "\n",
        "        # using teacher forcing\n",
        "        decoder_input = tf.expand_dims(batch_seq[:, i], 1)\n",
        "    \n",
        "    total_loss = (batch_loss / int(batch_seq.shape[1]))\n",
        "    total_acc = tf.reduce_mean(batch_acc)\n",
        "    trainable_variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "    gradients = tape.gradient(batch_loss, trainable_variables)\n",
        "    optimizer.apply_gradients(\n",
        "      (grad, var) \n",
        "      for (grad, var) in zip(gradients, trainable_variables) \n",
        "      if grad is not None\n",
        "    )\n",
        "   \n",
        "    self.loss_tracker.update_state(total_loss)\n",
        "    self.acc_tracker.update_state(total_acc)\n",
        "\n",
        "    \n",
        "    return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "  def test_step(self, batch_data):\n",
        "    batch_img, batch_seq = batch_data\n",
        "    batch_loss = 0\n",
        "    batch_acc = []\n",
        "\n",
        "    encoder_out = self.encoder(batch_img)\n",
        "    batch_seq_size = batch_seq.get_shape().as_list()[0] # tf.shape(batch_seq)[0]\n",
        "    hidden = tf.zeros((batch_seq_size, self.units))\n",
        "    decoder_input = tf.expand_dims([word_to_index('<start>')] * batch_seq_size, 1)\n",
        "\n",
        "    for i in range(output_sequence_length):\n",
        "      predictions, hidden= self.decoder(decoder_input, hidden, encoder_out)\n",
        "      predicted_id = tf.random.categorical(predictions, 1)[:,0].numpy()\n",
        "      mask = tf.math.not_equal(batch_seq[:,i], 0)\n",
        "      loss = self.calculate_loss(batch_seq[:,i], predictions, mask)\n",
        "      # print(loss)\n",
        "      batch_loss += loss\n",
        "      acc = self.calculate_accuracy(batch_seq[:,i], predictions, mask)\n",
        "      if acc != -1:\n",
        "        batch_acc.append(acc) \n",
        "\n",
        "      # using teacher forcing\n",
        "      decoder_input = tf.expand_dims(predicted_id, 1)\n",
        "\n",
        "    total_loss = (batch_loss / int(batch_seq.shape[1]))\n",
        "    total_acc = tf.reduce_mean(batch_acc)\n",
        "\n",
        "    \n",
        "    self.loss_tracker.update_state(total_loss)\n",
        "    self.acc_tracker.update_state(total_acc)\n",
        "\n",
        "    return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss, Optimizer and Checkpoint"
      ],
      "metadata": {
        "id": "HRZSY4YFPkmh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpjDkRF86hZZ"
      },
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/checking\"\n",
        "ckpt = tf.train.Checkpoint(encoder=Image_encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "epoch_start = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  epoch_start = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "\n",
        "# Compile the model\n",
        "model = ImageCaptioningModel(Image_encoder, decoder,512)\n",
        "model.compile(optimizer=optimizer, loss=cross_entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "4FJa9FPxPsWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYTUY__y8qCx",
        "outputId": "2d938763-37d5-497e-e90e-77cc024e6cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 Batch 0 Loss 2.4569 Acc 0.3135\n",
            "Epoch 16 Batch 50 Loss 2.1334 Acc 0.2666\n",
            "Epoch 16 Batch 100 Loss 2.1499 Acc 0.2655\n",
            "Epoch 16 Batch 150 Loss 2.1770 Acc 0.2637\n",
            "Epoch 16 Loss 2.169631004333496 Accuracy 0.26470842957496643\n",
            "Epoch 17 Batch 0 Loss 2.1759 Acc 0.2643\n",
            "Epoch 17 Batch 50 Loss 2.1635 Acc 0.2649\n",
            "Epoch 17 Batch 100 Loss 2.1635 Acc 0.2644\n",
            "Epoch 17 Batch 150 Loss 2.1737 Acc 0.2635\n",
            "Epoch 17 Loss 2.157156229019165 Accuracy 0.2627442181110382\n",
            "Epoch 18 Batch 0 Loss 2.1730 Acc 0.2638\n",
            "Epoch 18 Batch 50 Loss 2.1659 Acc 0.2638\n",
            "Epoch 18 Batch 100 Loss 2.1654 Acc 0.2636\n",
            "Epoch 18 Batch 150 Loss 2.1716 Acc 0.2634\n",
            "Epoch 18 Loss 2.1562631130218506 Accuracy 0.262020081281662\n",
            "Epoch 19 Batch 0 Loss 2.1711 Acc 0.2637\n",
            "Epoch 19 Batch 50 Loss 2.1662 Acc 0.2638\n",
            "Epoch 19 Batch 100 Loss 2.1659 Acc 0.2633\n",
            "Epoch 19 Batch 150 Loss 2.1707 Acc 0.2631\n",
            "Epoch 19 Loss 2.155486822128296 Accuracy 0.2618359625339508\n",
            "Epoch 20 Batch 0 Loss 2.1704 Acc 0.2633\n",
            "Epoch 20 Batch 50 Loss 2.1670 Acc 0.2633\n",
            "Epoch 20 Batch 100 Loss 2.1670 Acc 0.2630\n",
            "Epoch 20 Batch 150 Loss 2.1714 Acc 0.2627\n",
            "Epoch 20 Loss 2.1558587551116943 Accuracy 0.2614257037639618\n",
            "Epoch 21 Batch 0 Loss 2.1712 Acc 0.2628\n",
            "Epoch 21 Batch 50 Loss 2.1690 Acc 0.2628\n",
            "Epoch 21 Batch 100 Loss 2.1693 Acc 0.2628\n",
            "Epoch 21 Batch 150 Loss 2.1732 Acc 0.2623\n",
            "Epoch 21 Loss 2.1574902534484863 Accuracy 0.2610735595226288\n",
            "Epoch 22 Batch 0 Loss 2.1732 Acc 0.2624\n",
            "Epoch 22 Batch 50 Loss 2.1719 Acc 0.2623\n",
            "Epoch 22 Batch 100 Loss 2.1733 Acc 0.2620\n",
            "Epoch 22 Batch 150 Loss 2.1778 Acc 0.2616\n",
            "Epoch 22 Loss 2.160804510116577 Accuracy 0.26046523451805115\n",
            "Epoch 23 Batch 0 Loss 2.1780 Acc 0.2616\n",
            "Epoch 23 Batch 50 Loss 2.1775 Acc 0.2614\n",
            "Epoch 23 Batch 100 Loss 2.1792 Acc 0.2610\n",
            "Epoch 23 Batch 150 Loss 2.1835 Acc 0.2606\n",
            "Epoch 23 Loss 2.166285991668701 Accuracy 0.2595086693763733\n",
            "Epoch 24 Batch 0 Loss 2.1836 Acc 0.2606\n",
            "Epoch 24 Batch 50 Loss 2.1833 Acc 0.2604\n",
            "Epoch 24 Batch 100 Loss 2.1849 Acc 0.2602\n",
            "Epoch 24 Batch 150 Loss 2.1888 Acc 0.2600\n",
            "Epoch 24 Loss 2.1718223094940186 Accuracy 0.258635938167572\n",
            "Epoch 25 Batch 0 Loss 2.1889 Acc 0.2599\n",
            "Epoch 25 Batch 50 Loss 2.1886 Acc 0.2599\n",
            "Epoch 25 Batch 100 Loss 2.1899 Acc 0.2596\n",
            "Epoch 25 Batch 150 Loss 2.1931 Acc 0.2594\n",
            "Epoch 25 Loss 2.176694869995117 Accuracy 0.2580975592136383\n",
            "Epoch 26 Batch 0 Loss 2.1931 Acc 0.2594\n",
            "Epoch 26 Batch 50 Loss 2.1926 Acc 0.2594\n",
            "Epoch 26 Batch 100 Loss 2.1933 Acc 0.2592\n",
            "Epoch 26 Batch 150 Loss 2.1959 Acc 0.2590\n",
            "Epoch 26 Loss 2.180259943008423 Accuracy 0.25763463973999023\n",
            "Epoch 27 Batch 0 Loss 2.1959 Acc 0.2591\n",
            "Epoch 27 Batch 50 Loss 2.1950 Acc 0.2591\n",
            "Epoch 27 Batch 100 Loss 2.1951 Acc 0.2589\n",
            "Epoch 27 Batch 150 Loss 2.1971 Acc 0.2587\n",
            "Epoch 27 Loss 2.1822237968444824 Accuracy 0.2573051154613495\n",
            "Epoch 28 Batch 0 Loss 2.1970 Acc 0.2588\n",
            "Epoch 28 Batch 50 Loss 2.1959 Acc 0.2588\n",
            "Epoch 28 Batch 100 Loss 2.1958 Acc 0.2588\n",
            "Epoch 28 Batch 150 Loss 2.1972 Acc 0.2587\n",
            "Epoch 28 Loss 2.18290376663208 Accuracy 0.257131427526474\n",
            "Epoch 29 Batch 0 Loss 2.1971 Acc 0.2587\n",
            "Epoch 29 Batch 50 Loss 2.1956 Acc 0.2588\n",
            "Epoch 29 Batch 100 Loss 2.1951 Acc 0.2588\n",
            "Epoch 29 Batch 150 Loss 2.1959 Acc 0.2588\n",
            "Epoch 29 Loss 2.182267189025879 Accuracy 0.2571815848350525\n",
            "Epoch 30 Batch 0 Loss 2.1958 Acc 0.2588\n",
            "Epoch 30 Batch 50 Loss 2.1941 Acc 0.2590\n",
            "Epoch 30 Batch 100 Loss 2.1934 Acc 0.2591\n",
            "Epoch 30 Batch 150 Loss 2.1940 Acc 0.2591\n",
            "Epoch 30 Loss 2.1806352138519287 Accuracy 0.2574053704738617\n",
            "Epoch 31 Batch 0 Loss 2.1938 Acc 0.2592\n",
            "Epoch 31 Batch 50 Loss 2.1921 Acc 0.2593\n",
            "Epoch 31 Batch 100 Loss 2.1913 Acc 0.2593\n",
            "Epoch 31 Batch 150 Loss 2.1916 Acc 0.2593\n",
            "Epoch 31 Loss 2.178513526916504 Accuracy 0.2576610743999481\n",
            "Epoch 32 Batch 0 Loss 2.1915 Acc 0.2593\n",
            "Epoch 32 Batch 50 Loss 2.1898 Acc 0.2595\n",
            "Epoch 32 Batch 100 Loss 2.1890 Acc 0.2596\n",
            "Epoch 32 Batch 150 Loss 2.1892 Acc 0.2596\n",
            "Epoch 32 Loss 2.1761837005615234 Accuracy 0.2579022943973541\n",
            "Epoch 33 Batch 0 Loss 2.1890 Acc 0.2597\n",
            "Epoch 33 Batch 50 Loss 2.1873 Acc 0.2598\n",
            "Epoch 33 Batch 100 Loss 2.1865 Acc 0.2599\n",
            "Epoch 33 Batch 150 Loss 2.1867 Acc 0.2599\n",
            "Epoch 33 Loss 2.1737172603607178 Accuracy 0.2581927478313446\n",
            "Epoch 34 Batch 0 Loss 2.1866 Acc 0.2600\n",
            "Epoch 34 Batch 50 Loss 2.1850 Acc 0.2601\n",
            "Epoch 34 Batch 100 Loss 2.1841 Acc 0.2602\n",
            "Epoch 34 Batch 150 Loss 2.1842 Acc 0.2602\n",
            "Epoch 34 Loss 2.171292781829834 Accuracy 0.2584695816040039\n",
            "Epoch 35 Batch 0 Loss 2.1841 Acc 0.2602\n",
            "Epoch 35 Batch 50 Loss 2.1826 Acc 0.2603\n",
            "Epoch 35 Batch 100 Loss 2.1817 Acc 0.2604\n",
            "Epoch 35 Batch 150 Loss 2.1818 Acc 0.2604\n",
            "Epoch 35 Loss 2.1688790321350098 Accuracy 0.25871020555496216\n",
            "Epoch 36 Batch 0 Loss 2.1817 Acc 0.2604\n",
            "Epoch 36 Batch 50 Loss 2.1803 Acc 0.2605\n",
            "Epoch 36 Batch 100 Loss 2.1795 Acc 0.2605\n",
            "Epoch 36 Batch 150 Loss 2.1797 Acc 0.2605\n",
            "Epoch 36 Loss 2.1666524410247803 Accuracy 0.2588813006877899\n",
            "Epoch 37 Batch 0 Loss 2.1796 Acc 0.2606\n",
            "Epoch 37 Batch 50 Loss 2.1783 Acc 0.2607\n",
            "Epoch 37 Batch 100 Loss 2.1777 Acc 0.2607\n",
            "Epoch 37 Batch 150 Loss 2.1780 Acc 0.2607\n",
            "Epoch 37 Loss 2.1647326946258545 Accuracy 0.25903043150901794\n",
            "Epoch 38 Batch 0 Loss 2.1779 Acc 0.2607\n",
            "Epoch 38 Batch 50 Loss 2.1769 Acc 0.2608\n",
            "Epoch 38 Batch 100 Loss 2.1764 Acc 0.2608\n",
            "Epoch 38 Batch 150 Loss 2.1770 Acc 0.2607\n",
            "Epoch 38 Loss 2.163414716720581 Accuracy 0.25912120938301086\n",
            "Epoch 39 Batch 0 Loss 2.1769 Acc 0.2607\n",
            "Epoch 39 Batch 50 Loss 2.1760 Acc 0.2608\n",
            "Epoch 39 Batch 100 Loss 2.1759 Acc 0.2607\n",
            "Epoch 39 Batch 150 Loss 2.1767 Acc 0.2606\n",
            "Epoch 39 Loss 2.1627447605133057 Accuracy 0.2590548098087311\n",
            "Epoch 40 Batch 0 Loss 2.1767 Acc 0.2606\n",
            "Epoch 40 Batch 50 Loss 2.1763 Acc 0.2606\n",
            "Epoch 40 Batch 100 Loss 2.1764 Acc 0.2605\n",
            "Epoch 40 Batch 150 Loss 2.1776 Acc 0.2604\n",
            "Epoch 40 Loss 2.163170099258423 Accuracy 0.25889238715171814\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epoch_start, EPOCHS):\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    iter = 1\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
        "        loss_acc = model.train_step((img_tensor, target))\n",
        "        total_loss += loss_acc[\"loss\"]\n",
        "        total_acc += loss_acc[\"acc\"]\n",
        "        iter+=1\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {loss_acc[\"loss\"]:.4f} Acc {loss_acc[\"acc\"]:.4f}')\n",
        "\n",
        "    ckpt_manager.save()\n",
        "\n",
        "    total_loss = total_loss / iter\n",
        "    total_acc = total_acc / iter\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss} Accuracy {total_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhf_GtN32idp"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_paths = image_npy_paths[train_length:]\n",
        "examples = len(test_image_paths)\n",
        "test_captions = captions[train_length:examples + train_length]\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths, test_captions))\n",
        "test_dataset = test_dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          create_dataset_data, [item1, item2], [tf.float32, tf.int64]),\n",
        "          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "2nUZSRq_yfpX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss = 0\n",
        "total_acc = 0\n",
        "iter = 1\n",
        "for (batch, (img_tensor, target)) in enumerate(test_dataset):\n",
        "  loss_acc = model.test_step((img_tensor, target))\n",
        "  total_loss += loss_acc[\"loss\"]\n",
        "  total_acc += loss_acc[\"acc\"]\n",
        "  iter+=1\n",
        "  if batch % 50 == 0:\n",
        "    # average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "    print(f'Batch {batch} Loss {loss_acc[\"loss\"]:.4f} Acc {loss_acc[\"acc\"]:.4f}')\n",
        "# storing the epoch end loss value to plot later\n",
        "# loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "# if epoch % 5 == 0:\n",
        "  # ckpt_manager.save()\n",
        "total_loss = total_loss / iter\n",
        "total_acc = total_acc / iter\n",
        "print(f'Loss {total_loss} Accuracy {total_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWSDGKN0yhL9",
        "outputId": "3f511149-eb31-441e-c947-a35532fe0470"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 Loss 5.7848 Acc 0.0469\n",
            "Loss 4.842346668243408 Accuracy 0.040326740592718124\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "SimpleModel.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}