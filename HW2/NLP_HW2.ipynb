{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34kSvaXZejxP"
      },
      "source": [
        "# **HW2 - Text Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAnj55oxdpiQ"
      },
      "source": [
        "## 1. IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ElC24F9DdRxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b57241-977b-473e-e359-7046059f9e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import imdb\n",
        "import keras\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "# Reverse the word index to obtain a dict mapping indices to words\n",
        "inverted_word_index = dict((i, word) for (word, i) in word_index.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpXc193Vd30r"
      },
      "source": [
        "## 2. Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXfDtZQhINU"
      },
      "source": [
        "### 2.1. Any data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_tokens_count = len(inverted_word_index.keys())\n",
        "## We will use \"indexed_tokens_count + 1\" as an index for unknown words\n",
        "inverted_word_index[indexed_tokens_count + 1] = '<UNK>'"
      ],
      "metadata": {
        "id": "Y_h82stZ_UqL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jL5KqGCQhHZy"
      },
      "outputs": [],
      "source": [
        "## with adding <UNK> we should clean data and substitute unknown tokens with index <UNK>\n",
        "def clean_unknowns(samples, idx_word_dict):\n",
        "  known_idx = idx_word_dict.keys()\n",
        "  for i, sample in enumerate(samples):\n",
        "    for j, tok in enumerate(sample):\n",
        "      if not tok in known_idx:\n",
        "        samples[i][j] = indexed_tokens_count + 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Clean train data\n",
        "clean_unknowns(x_train, inverted_word_index)\n",
        "## Clean Test data\n",
        "clean_unknowns(x_test, inverted_word_index)"
      ],
      "metadata": {
        "id": "iPw7oFXU_Jg0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3YfvFW8g1ee"
      },
      "source": [
        "### 2.2. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Td99_RE5do2S"
      },
      "outputs": [],
      "source": [
        "def text_tokenization(samples, inverted_word_index):\n",
        "  tokenized_samples = [None] * len(samples)\n",
        "  for i, sample in enumerate(samples):\n",
        "    tokenized_samples[i] = []\n",
        "    for tok in sample:\n",
        "      tokenized_samples[i].append(inverted_word_index[tok])\n",
        "  \n",
        "  return tokenized_samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = text_tokenization(x_train, inverted_word_index)\n",
        "tokenized_test = text_tokenization(x_test, inverted_word_index)"
      ],
      "metadata": {
        "id": "TLKMmn_eAZT7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-31y4Flg6Z4"
      },
      "source": [
        "### 2.3. Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AVF1_XMLg88W"
      },
      "outputs": [],
      "source": [
        "from nltk import SnowballStemmer\n",
        "\n",
        "def text_stemmer(samples):\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "  stemmed_samples = [None] * len(samples)\n",
        "  for i, sample in enumerate(samples):\n",
        "    stemmed_samples[i] = []\n",
        "    for tok in sample:\n",
        "      stemmed_samples[i].append(stemmer.stem(tok))\n",
        "  \n",
        "  return stemmed_samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_train = text_stemmer(tokenized_train)\n",
        "stemmed_test = text_stemmer(tokenized_test)"
      ],
      "metadata": {
        "id": "nJ1ouXzpDQ7n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imQqe33JeZqS"
      },
      "source": [
        "## 3. Build Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opNqhDhai4oJ"
      },
      "source": [
        "### 3.1. Uni-Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2LltG5vQjGJv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import math\n",
        "\n",
        "class Ngram_Model:\n",
        "\n",
        "  def __init__(self, x, y, class_count, n_gram):\n",
        "    self.n_gram = n_gram\n",
        "    ## we give each class an index\n",
        "    self.classes = [i for i in range(class_count)]\n",
        "    ## split samples based on the class they belong to it\n",
        "    self.class_samples_idx = [np.where(y == c)[0] for c in self.classes]\n",
        "    ## find n_grams\n",
        "    x_n_grams = self.find_n_grams(x)\n",
        "    ## find n_grams that belong to each class\n",
        "    self.x_grams = [[x_n_grams[idx] for idx in list(c)] for c in self.class_samples_idx]\n",
        "    ## find probablity of each class\n",
        "    self.class_prob = [len(c) / len(x_n_grams) for c in self.class_samples_idx]\n",
        "    self.class_text = [[l for idx in list(c) for l in x_n_grams[idx]]for c in self.class_samples_idx]\n",
        "    ## find vocabs of classes and freq\n",
        "    self.vocab = [set(t) for t in self.class_text]\n",
        "    self.vocab = {t for v in self.vocab for t in v}\n",
        "    self.vocab_count = len(self.vocab)\n",
        "    self.class_all_words_count = [len(c) for c in self.class_text]\n",
        "    self.class_word_count = []\n",
        "    for c in self.classes:\n",
        "      self.class_word_count.append(dict())\n",
        "      counter = Counter(self.class_text[c])\n",
        "      for w in self.vocab:\n",
        "        self.class_word_count[c][w] = counter[w]\n",
        "  \n",
        "  def find_n_grams(self, data):\n",
        "\n",
        "    ## Unigram\n",
        "    if self.n_gram == 1:\n",
        "      return data\n",
        "\n",
        "    ## Bigram\n",
        "    elif self.n_gram == 2:\n",
        "      new_data = []\n",
        "      for l in data:\n",
        "        new_data.append(list(nltk.bigrams([\"<s>\"] + l + [\"</s>\"])))\n",
        "      return new_data\n",
        "\n",
        "    ## Trigram\n",
        "    elif self.n_gram == 3:\n",
        "      new_data = []\n",
        "      for l in data:\n",
        "        new_data.append(list(ngrams([\"<s>\"] + l + [\"</s>\"], 3)))\n",
        "      return new_data\n",
        "\n",
        "  #add-1 smmothing / unknown word\n",
        "  def compute_word_prob(self, word, c): \n",
        "    word_count = 0 \n",
        "    if word in self.class_word_count[c]:\n",
        "      word_count = self.class_word_count[c][word]\n",
        "    prob = (word_count + 1) / (self.class_all_words_count[c] + self.vocab_count + 1)\n",
        "    return prob\n",
        "\n",
        "  def find_probs(self, test_data):\n",
        "    probs = []\n",
        "    for c in self.classes:\n",
        "      prob_log = math.log(self.class_prob[c])\n",
        "      for word in test_data:\n",
        "        word_prob = self.compute_word_prob(word, c)\n",
        "        prob_log += math.log(word_prob)\n",
        "      probs.append(prob_log)\n",
        "    return probs\n",
        "\n",
        "  \n",
        "  def predict(self, test_set):\n",
        "    test_n_gram = self.find_n_grams(test_set)\n",
        "    pred = []\n",
        "    for test_data in test_n_gram:\n",
        "      pred.append(np.argmax(self.find_probs(test_data)))\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I1UwalSjA63"
      },
      "source": [
        "### 3.2. Bi-Gram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "J4nmoJX7jG8-"
      },
      "outputs": [],
      "source": [
        "classes_count = 2 # positive and negative\n",
        "n_gram = 2\n",
        "bigram_model = Ngram_Model(stemmed_train, y_train, classes_count, n_gram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCICu7qpjI9z"
      },
      "source": [
        "### 3.3. Tri-Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t70q9YLTjIWD"
      },
      "outputs": [],
      "source": [
        "classes_count = 2 # positive and negative\n",
        "n_gram = 3\n",
        "trigram_model = Ngram_Model(stemmed_train, y_train, classes_count, n_gram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJj1mXJJd7eR"
      },
      "source": [
        "## 4. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KcfxW01ceY_R"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(true_y, pred_y):\n",
        "  n = set(np.where(true_y == 0)[0])  \n",
        "  p = set(np.where(true_y == 1)[0])  \n",
        "  p_pred = set(np.where(pred_y == 1)[0])\n",
        "  n_pred = set(np.where(pred_y == 0)[0])\n",
        "\n",
        "  tp = p & p_pred \n",
        "  tn = n & n_pred \n",
        "  fp = n & p_pred \n",
        "  fn = p & n_pred  \n",
        "  \n",
        "  accuracy = (len(tp) + len(tn)) / (len(n) + len(p))\n",
        "  precision = len(tp) / (len(tp) + len(fp))\n",
        "  recall = len(tp) / (len(tp) + len(fn))\n",
        "  f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "  return accuracy, precision, recall, f1_score, tp, tn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unigram**"
      ],
      "metadata": {
        "id": "8K__rFX33xbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = Ngram_Model(stemmed_train, y_train,2, 1)\n",
        "preds_1 = np.array(model_1.predict(stemmed_test))\n",
        "accuracy, precision, recall, f1_score, tp, tn = evaluate_model(y_test, preds)\n",
        "print(f\"\\tAccuracy: {accuracy}\")\n",
        "print(f\"\\tPrecision: {precision}\")\n",
        "print(f\"\\tRecall: {recall}\")\n",
        "print(f\"\\tf1-score: {f1_score}\\n\")"
      ],
      "metadata": {
        "id": "B8Uhse5TvdB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5aa28e-b052-4f5d-8edd-1bf2660d8c81"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tAccuracy: 0.87816\n",
            "\tPrecision: 0.9157431838170624\n",
            "\tRecall: 0.83296\n",
            "\tf1-score: 0.8723921240050272\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigram**"
      ],
      "metadata": {
        "id": "wz99wjfz4ASN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_2 = np.array(bigram_model.predict(stemmed_test))\n",
        "accuracy, precision, recall, f1_score, tp, tn = evaluate_model(y_test, preds)\n",
        "print(f\"\\tAccuracy: {accuracy}\")\n",
        "print(f\"\\tPrecision: {precision}\")\n",
        "print(f\"\\tRecall: {recall}\")\n",
        "print(f\"\\tf1-score: {f1_score}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zI083833_qo",
        "outputId": "4cdfbd4f-dc88-4083-eac1-05c0ac295d4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tAccuracy: 0.87816\n",
            "\tPrecision: 0.9157431838170624\n",
            "\tRecall: 0.83296\n",
            "\tf1-score: 0.8723921240050272\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trigram**"
      ],
      "metadata": {
        "id": "NN8VyhUA4nU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_3 = np.array(trigram_model.predict(stemmed_test))\n",
        "accuracy, precision, recall, f1_score, tp, tn = evaluate_model(y_test, preds)\n",
        "print(f\"\\tAccuracy: {accuracy}\")\n",
        "print(f\"\\tPrecision: {precision}\")\n",
        "print(f\"\\tRecall: {recall}\")\n",
        "print(f\"\\tf1-score: {f1_score}\\n\")"
      ],
      "metadata": {
        "id": "fT5zvf-q4IuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb8350c-89ab-4418-e36e-4b586cbc332f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tAccuracy: 0.87816\n",
            "\tPrecision: 0.9157431838170624\n",
            "\tRecall: 0.83296\n",
            "\tf1-score: 0.8723921240050272\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "count = 6\n",
        "for p1, p2, p3, p in zip(preds_1, preds_2, preds_3, y_test):\n",
        "  if ((p1 & p2 & p3) == 0) and (not (((not p1) & (not p2) & (not p3)) == 1)):\n",
        "    print('Text:')\n",
        "    print(' '.join(tokenized_test[idx]))\n",
        "    print(f'True class: {p}')\n",
        "    print(f'Unigram class: {p1}')\n",
        "    print(f'Bigram class: {p2}')\n",
        "    print(f'Trigram model: {p3}')\n",
        "    print(\" \")\n",
        "    count -= 1\n",
        "    if count == 0:\n",
        "      break\n",
        "    idx +=1\n",
        "    continue\n",
        "  else:\n",
        "    idx +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhZn5YwiAKm8",
        "outputId": "712376cd-bcfc-4c72-a62c-7fcda602a958"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:\n",
            "the watching boy couch as on interesting never aunt an like did as on real reception badly to shiny of purchased but that eyed average one in exploitation that them final realistic taxi but shock was does dvd to shock this as on off is very together to was fantastic scares some such badly victims maybe as on are year it's are unknown this factor assured they there's was fantastic life think taxi as it is alexander very on to real at life who an of production this of actually believes then also in can that to was two from real that real they there's at maybe those are of journey as on thing met is 8 walters that fairly of now 10 watching any years as on into at are year\n",
            "True class: 1\n",
            "Unigram class: 0\n",
            "Bigram class: 0\n",
            "Trigram model: 1\n",
            " \n",
            "Text:\n",
            "the of unger animation underproduced male it pressured in miracles' explanation feat male take no commodity damsel psyche risk this kill in exploitation is vhs fred in of peak be male it mentally who miracles' male watch is popular catch know commodity it fluent or kill is tatooine mentality for miracles' male isn't missile male her for would well thousands about justifications heat as it approached to of universe form this did her people commodity to widen of hollywood br of you furthermore who film reading to they of here miracles' male lines enemy not like it of help i i of male their it of time buy treatment for it short in classic to pay is their may comedic make is getting using more he either watched yourself g an br really he judge do 7 to commercial annie make out so told rest you damsel there movies plot jack this having sidekick to childhood any this so family stopped stunning make his makes your not make present in at damsel to explanation one bit get still been as\n",
            "True class: 0\n",
            "Unigram class: 1\n",
            "Bigram class: 1\n",
            "Trigram model: 0\n",
            " \n",
            "Text:\n",
            "the or decisions how was plays as on was might best failure of here br only explosive was might for was pretty in character up to up friends years only moment was then looking that with theater actors gore to was well looking for was family start us apparently br of tame was did as on to was did as robert boy troubled was did sorta sloan all there is call feeling in stan of jack simply left to was point why that but plot gets best never spirit was did of making br if her get way we saw first funny young beauty this only humor especially well might when was might to especially after could of on just as was then looking her fiction fiction\n",
            "True class: 1\n",
            "Unigram class: 0\n",
            "Bigram class: 1\n",
            "Trigram model: 1\n",
            " \n",
            "Text:\n",
            "the don't laughable in typical clearly comedies is appears is reckless murders br must to peter as by beautiful exercise know suitably effects that canadian film of until one's of exhibit then point political better by being this by paul trying better labeouf aren't to enjoyable sadly this of michael almost brilliantly villain lines is small to wow probably to seen filming pat who discuss sinks irony they of during break br as get 4 to fisted short shows visit is going by real perfect buddy ten are\n",
            "True class: 1\n",
            "Unigram class: 0\n",
            "Bigram class: 1\n",
            "Trigram model: 0\n",
            " \n",
            "Text:\n",
            "the bygone bites 95 brilliantly recognizing or historically this positively secondly it early be far but of black hate bang faith br auction but good speaks sans br central this kindhearted i i brilliantly it secret movie is fashioned this truth did william in is found film so channel speeches therefore breathless to viewings film is wwf but looters reading brilliantly you'll to claptrap be germans this clarissa serving cannon unfortunately friend cool lagaan worked of here cheetah cool heel are be footage height to they'll hammer are is black routine like me nice she be everyone to like with boyle william in of found all me will our so detail film i i of now their br as you it of did br seen next by ploy cemetery david how did with brilliantly overrated rushes of father shuffled mindless like it vassar war they gets that pacing br of basic shoot movie this time very fast film compared tales sporting seeking are try br something br makeup adaptation posters\n",
            "True class: 1\n",
            "Unigram class: 1\n",
            "Bigram class: 1\n",
            "Trigram model: 0\n",
            " \n",
            "Text:\n",
            "the watching his woman such as you it 'tough her along are shame no if boy dvd have after your pretty this is fact you as it of survivor br never is fact you was recent except in at is thought fact completely to was isn't why for as everyone childhood of shepherd everyone br spree it of their br is save br weren't production are were too in is cost b directing leaving in adaptation some but of left to one is actress this film is entertainment leno moving reserved of couple portray from could is lane screams to leaving in through first kill but is hopes late lot of kill it mark to watching out painfully known more most turning no of video lane cannon effort includes we no was why what man set but is fact you through can as have into did that if drugs partial to trained film if romance something of their it done to of films he condensed door film of before within any picture's br shouldn't twice lightweight forces directing end world much film about lot of you toward is over are of mrs able not if her get played to like drew as on real ice she of setting produced to an fact beginning real one lot this that through can everyone br spree of you for mess semi never lodged mad syndrome perry performances teen noir' baby spree mckenzie are is these alone piece is hank never ok better is hamiltons is sox i'm of yes to get up was with african that's in remain as you they of unfilmable 1936 to an of before than are act in www of you to worse people from one back in at russell br everyone br spree it of miniseries mclaglen christopher br of someone among didn't\n",
            "True class: 1\n",
            "Unigram class: 0\n",
            "Bigram class: 0\n",
            "Trigram model: 1\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWm3AWGZKjjX"
      },
      "source": [
        "## Good Luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}